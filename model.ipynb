{"cells":[{"metadata":{"trusted":true,"_uuid":"634249637dd914be1d7a03c4a474d7d86d63d049"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport time\nimport os\nimport gc ## gabage collector\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import concatenate\nfrom keras.callbacks import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc2aef34024bc895752f52b31b3a46a2bad782bd"},"cell_type":"markdown","source":"**Load the training and testing dataset**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n#train_df['question_text'][train_df['target']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c29d913719c7c1cdbc00aa9809314a3f2e0b4f39"},"cell_type":"code","source":"embed_size = 300 \nvocab_size = 95000  # number of unique words in corpus\ninput_size = 70 # length of input questions","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## fill up missing values\nfilled_train = train_df[\"question_text\"].fillna(\"_##_\").values\nfilled_test = test_df[\"question_text\"].fillna(\"_##_\").values\n\n## Tokenise\nt = Tokenizer(num_words = vocab_size)\nt.fit_on_texts(list(filled_train))\n\n## encode the sequences\nencoded_train = t.texts_to_sequences(filled_train)\nencoded_test = t.texts_to_sequences(filled_test)\n\n## pad the sequences\npadded_train = pad_sequences(encoded_train, maxlen=input_size)\npadded_test = pad_sequences(encoded_test, maxlen=input_size)\n\n## labels\nlabels = train_df[\"target\"].values\n\n## shuffle the dataset\nnp.random.seed(2018)\nshuffled_index = np.random.permutation(len(padded_train))\ntrain = padded_train[shuffled_index]\nlabels = labels[shuffled_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f92c18f104bb6c539281428d35476e41de97aa24"},"cell_type":"markdown","source":"**Load the embedding GloVe**\n"},{"metadata":{"trusted":true,"_uuid":"bafbb7a6751c26969176e1b36e0c9d4445756464"},"cell_type":"code","source":"def load_embedding(url):\n    embeddings_index = dict()\n    f = open(url)\n    for line in f:\n        values = line.split(\" \")\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()    \n    embedding_matrix = np.zeros((vocab_size, embed_size))\n    for word, i in t.word_index.items():\n        if i >= vocab_size: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    ## release memory\n    del embeddings_index\n    import gc; gc.collect()\n    time.sleep(10)\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be91c417ddc5e8394da24c6ba6c5434281f49ecd","scrolled":true},"cell_type":"code","source":"glove_matrix = load_embedding('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n#wiki_matrix = load_embedding('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec')\n#para_matrix = load_embedding('../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt')\n#embedding_matrix = np.mean([glove_matrix, para_matrix], axis = 0)\nembedding_matrix = glove_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ca38705852b8afa3634587685992f05bac2f61"},"cell_type":"markdown","source":"**Building the model**"},{"metadata":{"trusted":true,"_uuid":"bafa7a4afc6fa2b72350a940075f464138999fdb"},"cell_type":"code","source":"# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37f864768a4909dadd851c176dea45184911670b"},"cell_type":"code","source":"## Input layer\ninp = Input(shape=(input_size,))\n\n## Embedding layer with dropout of 20 percents\nx = Embedding(vocab_size, embed_size, weights=[embedding_matrix], input_length=input_size, trainable=False)(inp)\nx = SpatialDropout1D(0.2)(x)\n\n## LSTM\nlstm = Bidirectional(CuDNNLSTM(input_size, return_sequences=True))(x)\natten_1 = Attention(input_size)(lstm)\nmax_pool_1 = GlobalMaxPooling1D()(lstm)\n#conc_1 = concatenate([max_pool_1, atten_1])\n#dropout_1 = Dropout(0.1)(max_pool_1)\n#dense_1 = Dense(input_size, activation='relu')(dropout_1)\n\n## GRU Layer\ngru = Bidirectional(CuDNNGRU(input_size, return_sequences=True))(x)\natten_2 = Attention(input_size)(gru)\nmax_pool_2 = GlobalMaxPooling1D()(gru)\n#conc_2 = concatenate([atten_2, max_pool_2])\n#dropout_2 = Dropout(0.1)(max_pool_2)\n#dense_2 = Dense(input_size, activation='relu')(dropout_2)\n\nconc1 = concatenate([atten_1, atten_2])\n#conc1 = GlobalMaxPooling1D()(conc1)\nconc1 = Dropout(0.1)(conc1)\n\nconc2 = concatenate([max_pool_1,max_pool_2])\n#conc2 = GlobalMaxPooling1D()(conc2)\nconc2 = Dropout(0.1)(conc2)\n\n\nconv1 = Conv1D(128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n#conv2 = GlobalMaxPooling1D()(conv1)\nconv3 = GlobalAveragePooling1D()(conv1)\n\nconc = concatenate([conc1, conc2, conv3])\nconc = Dropout(0.2)(conc)\nconc = Dense(input_size, activation='relu')(conc)\nconc = Dropout(0.1)(conc)\noutp = Dense(1, activation='sigmoid')(conc)\n\nmodel = Model(inputs = inp, outputs = outp)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['acc'])\n\n# summarize the model\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28e28f34da73dd8459ae36b9cbcd570d0e1065b6"},"cell_type":"code","source":"permuted_index = np.random.permutation(len(train))\npercentage = 0.2 # percentage of data used for validation\ncutoff = int(len(train)*percentage)\ntrain_index = permuted_index[cutoff:]\nval_index = permuted_index[:cutoff]\n\n## training and labels\ntrains = train[train_index]\ntrain_labels = labels[train_index]\n\n## validation set\nval = train[val_index]\nval_labels = labels[val_index]\n\nprint(len(train))\nprint(len(trains)+len(val))\nprint(len(train_labels)+len(val_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91c11a68915f1696d7e586aa88406d3727f6ae12","scrolled":false},"cell_type":"code","source":"# fit the model\nmodel.fit(trains, train_labels, batch_size=512, epochs=20, verbose=1, callbacks=None, validation_data=(val, val_labels), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b98204ffd86e8c731f6a9190ca16fbade0145f1"},"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(val, val_labels, verbose=1)\nprint('Accuracy: %f' % (accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a0c4e537351884beebd4f51fe6ef5d59c946fd2"},"cell_type":"markdown","source":"**Choose threshold **"},{"metadata":{"trusted":true,"_uuid":"137a5f5f0a2d80e01e7fd1ad3da9b99f2601c76d"},"cell_type":"code","source":"pred_val = model.predict([val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = f1_score(val_labels, (pred_val>thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"060a9ffb3fa29146391d0da43834d643e59cb07b"},"cell_type":"markdown","source":"**Run on test data**"},{"metadata":{"trusted":true,"_uuid":"2f0270faff17b4d3fe9e35f984709ebc14c2f7c2"},"cell_type":"code","source":"pred_test = model.predict(padded_test, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60100e7090d15f25ed59823dc90dc93b3ba8b154"},"cell_type":"code","source":"a = []\nfor line in pred_test:\n    a.append((line >= best_thresh).astype(int)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c6b9b9e9b1c8963f05c9c3b65024a4d64625bd5"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c3c6ed4417a21db8767718e6e22ef507af819df"},"cell_type":"code","source":"#sub = pd.read_csv('../input/sample_submission.csv')\nsub = pd.DataFrame()\nsub['qid'] = test_df['qid'].values\nsub['prediction'] = a\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}